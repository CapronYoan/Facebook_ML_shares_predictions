{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced ML prediction trades on internet compagny shares\n",
    "- data importation\n",
    "- data processing\n",
    "- simple regressor model\n",
    "- advanced optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas_datareader import data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2012-08-18'\n",
    "end_date = '2019-12-31'\n",
    "\n",
    "google_data = data.get_data_yahoo('GOOGL', start_date, end_date)\n",
    "facebook_data = data.get_data_yahoo('FB', start_date, end_date)\n",
    "alibaba_data = data.get_data_yahoo('BABA', start_date, end_date)\n",
    "baidu_data = data.get_data_yahoo('BIDU', start_date, end_date)\n",
    "sap_data = data.get_data_yahoo('SAP', start_date, end_date)\n",
    "saleforce_data = data.get_data_yahoo('CRM', start_date, end_date)\n",
    "vmware_data = data.get_data_yahoo('VMW', start_date, end_date)\n",
    "adobe_data = data.get_data_yahoo('ADBE', start_date, end_date)\n",
    "intuit_data = data.get_data_yahoo('INTU', start_date, end_date)\n",
    "twitter_data = data.get_data_yahoo('TWTR', start_date, end_date)\n",
    "paypal_data = data.get_data_yahoo('PYPL', start_date, end_date)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple data processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame({\"google_returns\": google_data[\"Close\"].shift(1) - google_data[\"Close\"], \"facebook_returns\": facebook_data[\"Close\"].shift(1) - facebook_data[\"Close\"], \"alibaba_returns\": alibaba_data[\"Close\"].shift(1) - alibaba_data[\"Close\"], \"baidu_returns\": baidu_data[\"Close\"].shift(1) - baidu_data[\"Close\"], \"sap_returns\": sap_data[\"Close\"].shift(1) - sap_data[\"Close\"], \"saleforce_returns\": saleforce_data[\"Close\"].shift(1) - saleforce_data[\"Close\"], \"vmware_returns\": vmware_data[\"Close\"].shift(1) - vmware_data[\"Close\"], \"adobe_returns\": adobe_data[\"Close\"].shift(1) - adobe_data[\"Close\"], \"intuit_returns\": intuit_data[\"Close\"].shift(1) - intuit_data[\"Close\"], \"twitter_returns\": twitter_data[\"Close\"].shift(1) - twitter_data[\"Close\"], \"paypal_returns\": paypal_data[\"Close\"].shift(1) - paypal_data[\"Close\"]})\n",
    "data_df.dropna(axis=0, inplace=True)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting datas with matplotlib\n",
    "data_df.cumsum().plot(figsize=(20, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation visualisation with seaborn heatmap\n",
    "import seaborn as sns\n",
    "sns.heatmap(data_df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set features as X and target as y with default features for this time\n",
    "X = data_df.drop([\"facebook_returns\"], axis=1)\n",
    "y = data_df[\"facebook_returns\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for split the datetime dataset, I use 2016 datas as train set and 2017 data as test set\n",
    "X_train, X_test, y_train, y_test = X[\"2016\"], X[\"2017\"], y[\"2016\"], y[\"2017\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple regressor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple ElasticNet model with default values\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "model = ElasticNet()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_train, y_train), model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Pandas DataFrame with target test set as y_test and model prediction of test features set\n",
    "comparison = pd.DataFrame({\"y_test\": y_test, \"prediction\": model.predict(X_test)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison.plot(figsize=(20, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify if the model is overfitted or underfitted with learning curve\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "N, train_score, val_score = learning_curve(model, X_train, y_train, train_sizes=np.linspace(0.2, 1.0, 5), cv=5)\n",
    "plt.plot(N, train_score.mean(axis=1), figsize=(20, 20))\n",
    "plt.plot(N, val_score.mean(axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced model optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I use GridSearchCV to cross validates differents parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\"alpha\": np.arange(0.1, 0.9, 0.1),\n",
    "              \"l1_ratio\": np.arange(0.1, 1., 0.1),\n",
    "              \"tol\": np.arange(0.00005, 0.0005, 0.00005)\n",
    "              }\n",
    "\n",
    "grid = GridSearchCV(ElasticNet(), param_grid, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameters got by GridSearchCV\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.score(X_train, y_train), grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Again I use learning curve to verify overfit or underfit\n",
    "N, train_score, val_score = learning_curve(grid, X_train, y_train, train_sizes=np.linspace(0.2, 1.0, 5), cv=5)\n",
    "plt.plot(N, train_score.mean(axis=1), figsize=(20, 20))\n",
    "plt.plot(N, val_score.mean(axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
